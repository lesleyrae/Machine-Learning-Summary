Time series 分解
首先，時間序列可以分解成以下三種特性：
1. stationary
中文譯為穩健性，表示時間序列資料的標準差以及平均值不會隨著時間推移而變化，而是保持著一個固定數值的狀態。stationary 是時間序列中非常著要的性質，有些預測的方法 (ARIMA) 根本上是基於此特性來去預測的。另一種 stationary 的說法為時序資料去除掉 Seasonality 與 trend 就會有 stationary 的性質了。
2. seasonality
表示時間序列中有相似的模式並週期性的循環出現。
3. trend
表示隨著時間的推移，時間序列有著明顯方向的增長或遞減。
而時間序列又可以分解成 additive 與 multiplicative 兩種不同的性質，兩者分解的公式如下：
* additive:
* yt = St + Tt + Et*
* multiplicative:
* yt = St x Tt x Et*
根據我對參考資料的理解，additive 特性的時間序列通常是代表有固定起伏、固定週期模式的線性時間序列。然而 multiplicative 則是相反，時間序列的起伏、週期會隨著時間不同成長幅度隨之變化，屬於非線性的時間序列，股票的時序資料多屬於此類。
Dataset A is multiplicative. We know this because magnitude of the seasonality component in the 1st plot above is dependent on the trend.
Dataset B is additive. We know this because the magnitude of the seasonality component in the 2nd plot above is independent of the trend.

What is Stationarity?

In order for time series data to be stationary, the data must exhibit four properties over time:
1. constant mean
2. constant variance
3. constant autocorrelation structure
To discuss these things simply we must introduce the idea of a lag. Suppose you wanted to know if today's stock price correlated better with yesterday's price or the price from two days ago. You could test this by computing the correlation between the original time series and the same series delayed one time interval. Therefore, the second value of the original time series would be compared with the first of the delayed. The third original value would be compared with the second of the delayed. And so on. Performing this process for a lag of 1 and a lag of 2, respectively, would yield two correlation outputs. This output would tell you which lag is more correlated. That is autocorrelation in a nutshell.
4. no periodic component

To check whether it is stationary?（Use ADF）
from statsmodels.tsa.stattools import adfuller
adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(stationary)
First, adf is the value of the test statistic. The more negative the value, the more confident we can be that the series is stationary. 
Next, pvalue is interpreted like any p-value. Once we set a threshold, we can compare this p-value to that threshold. Either we reject or fail to reject the null. Here pvalue is very close to zero (~10−1710−17) so we reject the null that this data is nonstationary.
The variable nobs is simply the number of observations in the time series, in this case 99.
Finally, the critical_values variable provides test statistic threholds for common significant levels. Here we see a test statistic of roughly -2.89 and lower is sufficient to reject the null using a significance level of 5%.

What is Smooothing?

Any data collection process is subject to noise. Oftentimes this noise can obscure useful patterns. Smoothing is a well-known and oft used technique to extract those patterns.
Smoothing comes in two flavors:
1. Simple
2. Exponential
Single Exponential Smoothing - no trend or seasonality
Double Exponential Smoothing - captures trend
Triple Exponential Smoothing - captures trend & seasonality

##single exponential
```
from statsmodels.tsa.api import SimpleExpSmoothing
single = SimpleExpSmoothing(train).fit(optimized=True)
single_preds = single.forecast(len(test))
single_mse = mse(test, single_preds)
print("Predictions: ", single_preds)
print("MSE: ", single_mse)
```
```
##double exponential
from statsmodels.tsa.api import Holt
double = Holt(train).fit(optimized=True)
double_preds = double.forecast(len(test))
double_mse = mse(test, double_preds)
print("Predictions: ", double_preds)
print("MSE: ", double_mse) 
```
```
##triple exponential
from statsmodels.tsa.api import ExponentialSmoothing
triple = ExponentialSmoothing(train,
                              trend="additive",
                              seasonal="additive",
                              seasonal_periods=13).fit(optimized=True)
triple_preds = triple.forecast(len(test))
triple_mse = mse(test, triple_preds)
print("Predictions: ", triple_preds)
print("MSE: ", triple_mse)
```
```
##comparison 
data_dict = {'MSE':[simple_mse, single_mse, double_mse, triple_mse]}
df = pd.DataFrame(data_dict, index=['simple', 'single', 'double', 'triple'])
print(df)
```
AR （Auto Regression）模型

（1）自回归模型是用自身的数据进行预测
（2）时间序列数据必须具有平稳性
（3）自回归只适用于预测与自身前期相关的现象（时间序列的自相关性）
Autocorrelation: a variable's correlation with itself at different lags.
AR models regress on actual past values
MA （Moving Average）模型
当  ，即时间序列当前值与历史值没有关系，而只依赖于历史白噪声的线性组合，就得到MA模型：

A MA model is defined by this equation: 𝑦𝑡=𝑐+𝑒𝑡+θ1𝑒𝑡−1+θ2𝑒𝑡−2+⋯+θ𝑞𝑒𝑡−𝑞yt=c+et+θ1et−1+θ2et−2+⋯+θqet−q where 𝑒𝑡et is white noise. The value 𝑐c is a constant value and the 𝜃θ's are coefficients, not unlike those found in linear regression.
ARMA 模型


将AR（p）与MA（q）结合，得到一个一般的自回归移动平均模型ARMA（p，q）：

（1）一个随机时间序列可以通过一个自回归移动平均模型来表示，即该序列可以由其自身的过去或滞后值以及随机扰动项来解释。
（2）如果该序列是平稳的，即它的行为并不会随着时间的推移而变化，那么我们就可以通过该序列过去的行为来预测未来。
* 流程：
* 对序列绘图，进行 ADF 检验，观察序列是否平稳；对于非平稳时间序列要先进行 d 阶差分，转化为平稳时间序列；
* 经过第一步处理，已经得到平稳时间序列。要对平稳时间序列分别求得其自相关系数（ACF）和偏自相关系数（PACF），通过对自相关图和偏自相关图的分析，得到最佳的阶数p、q；
* 由以上得到的d、q、p ，得到 ARIMA 模型。然后开始对得到的模型进行模型检验。
fig1 = sm.tsa.graphics.plot_acf(auto_1, lags=range(1,15), alpha=0.05)
fig2 = sm.tsa.graphics.plot_pacf(auto_2, lags=range(1,15), alpha=0.05)
拖尾和截尾
拖尾指序列以指数率单调递减或震荡衰减，而截尾指序列从某个时点变得非常小
截尾是指时间序列的自相关函数（ACF）或偏自相关函数（PACF）在某阶后均为0的性质（比如AR的PACF）；拖尾是ACF或PACF并不在某阶后均为0的性质（比如AR的ACF）。


图中自相关系数拖着长长的尾巴，就是拖尾，AC值是慢慢减少的。而偏相关系数是突然收敛到临界值水平范围内的，这就是截尾，PAC突然变的很小。

截尾


可以看到ACF是一个逐渐趋于0的拖尾，而PACF在7阶过后系数为0，所以模型是AR(7)或AR(8)，即ARMA(7,0）或ARMA（8,0)。



如果你的图片成这样，估计十有八九是一个ARMA模型了。自相关7阶拖尾（n从7开始缩至置信区间），偏自相关2阶拖尾。

But subjective
 ，其中 L 表示模型的极大似然函数， K 表示模型参数个数。
 ，其中 n 表示样本容量。
这两个评价指标越小越好。我们通过网格搜索，确定 AIC、BIC 最优的模型（p、q）。


SARIMA
We went through getting stationary data and differencing as that is the last piece of the puzzle that we are missing for understanding ARIMA models. The I stands for "Integrated" which just refers to the amount of differcing done on the data.
When we are determining our ARIMA model we will come across the following standard inputs:
* order(p,d,q):
    * p is number of AR terms
    * d is number of times that we would difference our data
    * q is number of MA terms
When we work with SARIMA models 'S' refers to 'seasonal' and we have the additional standard inputs:
* seasonal order(p,d,q):
    * p is number of AR terms in regards to seasonal lag
    * d is number of times that we would difference our seasonal lag (as seen above)
    * q is number of MA terms in regards to seasonal lag
    * s is number of periods in a season




##寻找最佳p,d,q
```

p = d = q = range(0, 2) 
pdq = list(itertools.product(p, d, q)) 
pdq_x_PDQs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))] 
a=[]
b=[]
c=[]
wf=pd.DataFrame()
for param in pdq:
    for seasonal_param in pdq_x_PDQs:
        try:
            mod = sm.tsa.statespace.SARIMAX(mte,order=param,seasonal_order=seasonal_param,enforce_stationarity=False,enforce_invertibility=False)
            results = mod.fit()
            print('ARIMA{}x{} - AIC:{}'.format(param, seasonal_param, results.aic))
            a.append(param)
            b.append(seasonal_param)
            c.append(results.aic)
        except:
            continue
wf['pdq']=a
wf['pdq_x_PDQs']=b
wf['aic']=c
print(wf[wf['aic']==wf['aic'].min()])
```
## prediction
```
mod = sm.tsa.statespace.SARIMAX(mte, 
                                order=(1,1,1), 
                                seasonal_order=(0,1,1,12),   
                                enforce_stationarity=False,
                                enforce_invertibility=False)
results = mod.fit()
print(results.summary())
```
#模型检验
#模型诊断
```
results.plot_diagnostics(figsize=(15, 12))
plt.show()
```
#LB检验
```
r,q,p = sm.tsa.acf(results.resid.values.squeeze(), qstat=True) 
data = np.c_[range(1,41), r[1:], q, p] 
table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"]) 
print(table.set_index('lag’))
```
##残差序列延迟1-12阶时，Q统计量的P值均大于0.05，所以在0.05的显著性水平下，不拒绝原假设，即残差为白噪声序列，说明残差序列的波动已经没有任何统计规律
